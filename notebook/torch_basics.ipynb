{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33235263",
   "metadata": {},
   "source": [
    "# Pytorch基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8c24d",
   "metadata": {},
   "source": [
    "## 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdcbd95",
   "metadata": {},
   "source": [
    "张量 (Tensor) 是深度学习的基础，例如常见的 0 维张量称为标量 (scalar)、1 维张量称为向量 (vector)、2 维张量称为矩阵 (matrix)。Pytorch 本质上就是一个基于张量的数学计算工具包，它提供了多种方式来创建张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05d8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364b8eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7160e-38, 6.4320e-43, 1.7160e-38],\n",
       "        [6.4320e-43, 1.7231e-38, 6.4320e-43]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2, 3) # empty tensor (uninitialized), shape (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3566ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4100, 0.8498, 0.4244],\n",
       "        [0.8438, 0.4283, 0.5325]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3) # random tensor, each value taken from [0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be0a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8111, -0.2812, -0.6018],\n",
       "        [ 1.0059,  0.7564, -1.8915]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3) # random tensor, each value taken from standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b249050d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, dtype=torch.long) # long integer zero tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2582f75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, dtype=torch.double) # double float zero tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77165d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b17e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[1.0, 3.8, 2.1], [8.6, 4.0, 2.4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8814b3d",
   "metadata": {},
   "source": [
    "也可以通过 `torch.tensor()` 或者 `torch.from_numpy()` 基于已有的数组或 Numpy 数组创建张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea70b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.8000, 2.1000],\n",
       "        [8.6000, 4.0000, 2.4000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07b6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e2e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1.0, 3.8, 2.1], [8.6, 4.0, 2.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843f0775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.8000, 2.1000],\n",
       "        [8.6000, 4.0000, 2.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67340fc",
   "metadata": {},
   "source": [
    "## 张量计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b0a10",
   "metadata": {},
   "source": [
    "张量的加减乘除是按元素进行计算的，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9166a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.double)\n",
    "y = torch.tensor([4, 5, 6], dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162f02b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d78e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3., -3., -3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3a8087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 10., 18.], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebd6ef70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48447388",
   "metadata": {},
   "source": [
    "Pytorch 还提供了许多常用的计算函数，如 torch.dot() 计算向量点积、torch.mm() 计算矩阵相乘、三角函数和各种数学函数等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a92499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32., dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ee223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8415, 0.9093, 0.1411], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7105e986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c01457",
   "metadata": {},
   "source": [
    "除了数学运算，Pytorch 还提供了多种张量操作函数，如聚合 (aggregation)、拼接 (concatenation)、比较、随机采样、序列化等，详细使用方法可以参见 [Pytorch 官方文档](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffbba0b",
   "metadata": {},
   "source": [
    "> 对张量进行聚合（如求平均、求和、最大值和最小值等）或拼接操作时，可以指定进行操作的维度 (dim)。例如，计算张量的平均值，在默认情况下会计算所有元素的平均值。："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6540baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec72a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d2866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5000, dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056936f",
   "metadata": {},
   "source": [
    "> 更常见的情况是需要计算某一行或某一列的平均值，此时就需要设定计算的维度，例如分别对第 0 维和第 1 维计算平均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3dc6673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 3.5000, 4.5000], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0965144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 5.], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83eb8d",
   "metadata": {},
   "source": [
    ">注意，上面的计算自动去除了多余的维度，因此结果从矩阵变成了向量，如果要保持维度不变，可以设置 keepdim=True："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c6edb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5000, 3.5000, 4.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "159a7083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [5.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ed38d",
   "metadata": {},
   "source": [
    "组合使用这些操作就可以写出复杂的数学计算表达式。例如对于\n",
    "$z = (x + y) \\times (y-2)$\n",
    "\n",
    "当 $x=2, y=3$ 时，很容易计算出 $z=5$。使用 Pytorch 来实现这一计算过程与 Python 非常类似，唯一的不同是数据使用张量进行保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7016182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.])\n",
    "y = torch.tensor([3.])\n",
    "z = (x + y) * (y - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45772977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d3a0f",
   "metadata": {},
   "source": [
    "使用 Pytorch 进行计算的好处是更高效的执行速度，尤其当张量存储的数据很多时，而且还可以借助 GPU 进一步提高计算速度。下面以计算三个矩阵相乘的结果为例，我们分别通过 CPU 和 NVIDIA Tesla V100 GPU 来进行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "698b85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4de0a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.7337969999644\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand(1000, 1000)\n",
    "print(timeit.timeit(lambda: M.mm(M).mm(M), number=5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.rand(1000, 1000).cuda()\n",
    "print(timeit.timeit(lambda: N.mm(N).mm(N), number=5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ed4b0",
   "metadata": {},
   "source": [
    "可以看到使用 GPU 能够明显地提高计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d55045",
   "metadata": {},
   "source": [
    "## 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207050a",
   "metadata": {},
   "source": [
    "Pytorch 提供自动计算梯度的功能，可以自动计算一个函数关于一个变量在某一取值下的导数，从而基于梯度对参数进行优化，这就是机器学习中的训练过程。使用 Pytorch 计算梯度非常容易，只需要执行 tensor.backward()，就会自动通过反向传播 (Back Propogation) 算法完成，后面我们在训练模型时就会用到该函数。\n",
    "\n",
    "注意，为了计算一个函数关于某一变量的导数，Pytorch 要求显式地设置该变量是可求导的，即在张量生成时，设置 `requires_grad=True`。我们对上面计算 $z = (x + y) \\times (y-2)$ 的代码进行简单修改，就可以计算当 $x=2, y=3$ 时，$\\frac {dz}{dx}$和 $ \\frac {dz}{dy}$的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2331b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d3e14be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04bf0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adc0dd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef8ab0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (x + y) * (y - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8319e55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74bdd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a81c961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091af83f",
   "metadata": {},
   "source": [
    "很容易手工求解 \n",
    " \n",
    " $\\frac {dz}{dx} = y-2$\n",
    "，$\\frac {dz}{dy} = x + 2y - 2$\n",
    "\n",
    "当 $x=2, y=3$ 时，\n",
    "$\\frac {dz}{dx} = 1$\n",
    " 和 \n",
    "$\\frac {dz}{dy} = 6$\n",
    "，与 Pytorch 代码计算结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a05415",
   "metadata": {},
   "source": [
    "## 调整张量形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c322c6",
   "metadata": {},
   "source": [
    "有时我们需要对张量的形状进行调整，Pytorch 共提供了 4 种调整张量形状的函数，分别为：\n",
    "- **形状转换** view 将张量转换为新的形状，需要保证总的元素个数不变，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e66dc732",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f035939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4042c977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c79db674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8d72a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b6b4e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5eac20",
   "metadata": {},
   "source": [
    "进行 view 操作的张量必须是连续的 (contiguous)，可以调用 `is_conuous` 来判断张量是否连续；如果非连续，需要先通过 `contiguous` 函数将其变为连续的。也可以直接调用 Pytorch 新提供的 `reshape` 函数，它与 `view` 功能几乎一致，并且能够自动处理非连续张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95054c4",
   "metadata": {},
   "source": [
    "- 转置 transpose 交换张量中的两个维度，参数为相应的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07affab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcf6dfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b704270f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2388d14",
   "metadata": {},
   "source": [
    "- 交换维度 permute 与 transpose 函数每次只能交换两个维度不同，permute 可以直接设置新的维度排列方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e56ec633",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1, 2, 3], [4, 5, 6]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54cc39a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b2e4f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "861ac724",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8e9c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 4]],\n",
       "\n",
       "        [[2, 5]],\n",
       "\n",
       "        [[3, 6]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98f01e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c504823",
   "metadata": {},
   "source": [
    "## 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffd0a1",
   "metadata": {},
   "source": [
    "前面我们都是假设参与运算的两个张量形状相同。在有些情况下，即使两个张量形状不同，也可以通过广播机制 (broadcasting mechanism) 对其中一个或者同时对两个张量的元素进行复制，使得它们形状相同，然后再执行按元素计算。\n",
    "\n",
    "例如，我们生成两个形状不同的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2537df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1, 4).view(3, 1)\n",
    "y = torch.arange(4, 6).view(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e07bb313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "881d5af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed969a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "995e44b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68482383",
   "metadata": {},
   "source": [
    "它们形状分别为 $(3,1)$ 和 $(1,2)$，如果要进行按元素运算，必须将它们都扩展为形状 $(3,2)$ 的张量。具体地，就是将 $x$ 的第 1 列复制到第 2 列，将 $y$ 的第 1 行复制到第 2、3 行。实际上，我们可以直接进行运算，Pytorch 会自动执行广播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77c9e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6],\n",
      "        [6, 7],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef38be",
   "metadata": {},
   "source": [
    "## 索引与切片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dda5c3",
   "metadata": {},
   "source": [
    "与 Python 列表类似，Pytorch 也可以对张量进行索引和切片。索引值同样是从 0 开始，切片 $[m:n]$ 的范围是从 $m$ 到 $n$ 前一个元素结束，并且可以对张量的任意一个维度进行索引或切片。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "739ec89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12).view(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "150e3f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f888682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1, 3] # element at row 1, column 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "756669a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6, 7])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1] # all elements in row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3f53ee31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:3] # elements in row 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a30d314b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  6, 10])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 2] # all elements in column 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55e97d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3],\n",
       "        [ 6,  7],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 2:4] # elements in column 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1410ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 2:4] = 100 # set elements in column 2 & 3 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a56c28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1, 100, 100],\n",
       "        [  4,   5, 100, 100],\n",
       "        [  8,   9, 100, 100]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6563c",
   "metadata": {},
   "source": [
    "## 降维与升维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdcd19",
   "metadata": {},
   "source": [
    "有时为了计算需要对一个张量进行降维或升维。例如神经网络通常只接受一个批次 (batch) 的样例作为输入，如果只有 1 个输入样例，就需要手工添加一个 batch 维度。具体地：\n",
    "\n",
    "- 升维 `torch.unsqueeze(input, dim, out=None)` 在输入张量的 dim 位置插入一维，与索引一样，dim 值也可以为负数；\n",
    "- 降维 `torch.squeeze(input, dim=None, out=None)` 在不指定 dim 时，张量中所有形状为 1 的维度都会被删除，例如 $(A,1,B,1C)$ 会变成 $(A,B,C)$ ；当给定 dim 时，只会删除给定的维度（形状必须为 1），例如对于 $(A,1,B)$ ，`squeeze(input, dim=0)` 会保持张量不变，只有 `squeeze(input, dim=1)` 形状才会变成 $(A,B)$。\n",
    "\n",
    "下面是一些示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1224e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b9aeaa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4a20ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.unsqueeze(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "031dda81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c92a5ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a14f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "98210e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3df6ef8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815cb6f8",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7588a8f",
   "metadata": {},
   "source": [
    "Pytorch 提供了 DataLoader 和 Dataset 类（或 IterableDataset）专门用于处理数据，它们既可以加载 Pytorch 预置的数据集，也可以加载自定义数据。其中数据集类 Dataset（或 IterableDataset）负责存储样本以及它们对应的标签；数据加载类 DataLoader 负责迭代地访问数据集中的样本。\n",
    "\n",
    "> https://transformers.run/c2/2021-12-14-transformers-note-3/#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2f7bd",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "数据集负责存储数据样本，所有的数据集类都必须继承自 Dataset 或 IterableDataset。具体地，Pytorch 支持两种形式的数据集："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c1f34",
   "metadata": {},
   "source": [
    "\n",
    "- 映射型 (Map-style) 数据集\n",
    "\n",
    "继承自 `Dataset` 类，表示一个从索引到样本的映射（索引可以不是整数），这样我们就可以方便地通过 `dataset[idx]` 来访问指定索引的样本。这也是目前最常见的数据集类型。映射型数据集必须实现 `__getitem__()` 函数，其负责根据指定的 key 返回对应的样本。一般还会实现 `__len__()` 用于返回数据集的大小。\n",
    "\n",
    ">`DataLoader` 在默认情况下会创建一个生成整数索引的索引采样器 (sampler) 用于遍历数据集。因此，如果我们加载的是一个非整数索引的映射型数据集，还需要手工定义采样器。\n",
    "\n",
    "- 迭代型 (Iterable-style) 数据集\n",
    "\n",
    "继承自 `IterableDataset`，表示可迭代的数据集，它可以通过 `iter(dataset)` 以数据流 (steam) 的形式访问，适用于访问超大数据集或者远程服务器产生的数据。 迭代型数据集必须实现 `__iter__()` 函数，用于返回一个样本迭代器 (iterator)。\n",
    "\n",
    ">注意：如果在 `DataLoader` 中开启多进程（num_workers > 0），那么在加载迭代型数据集时必须进行专门的设置，否则会重复访问样本。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a6f89b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, start, end):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        assert end > start\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.end))\n",
    "\n",
    "ds = MyIterableDataset(start=3, end=7) # [3, 4, 5, 6]\n",
    "# Single-process loading\n",
    "print(list(DataLoader(ds, num_workers=0)))\n",
    "# Directly doing multi-process loading\n",
    "print(list(DataLoader(ds, num_workers=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b729d",
   "metadata": {},
   "source": [
    "可以看到，当 DataLoader 采用 2 个进程时，由于每个进程都获取到了单独的数据集拷贝，因此会重复访问每一个样本。要避免这种情况，就需要在 DataLoader 中设置 worker_init_fn 来自定义每一个进程的数据集拷贝："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed02468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import get_worker_info\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_info = get_worker_info()\n",
    "    dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "    overall_start = dataset.start\n",
    "    overall_end = dataset.end\n",
    "    # configure the dataset to only process the split workload\n",
    "    per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "    worker_id = worker_info.id\n",
    "    dataset.start = overall_start + worker_id * per_worker\n",
    "    dataset.end = min(dataset.start + per_worker, overall_end)\n",
    "\n",
    "# Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
    "# print(list(DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n",
    "# With even more workers\n",
    "# print(list(DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49513981",
   "metadata": {},
   "source": [
    "下面我们以加载一个图像分类数据集为例，看看如何创建一个自定义的映射型数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d40dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd2127",
   "metadata": {},
   "source": [
    "可以看到，我们实现了 `__init__()`、`__len__()` 和 `__getitem__()` 三个函数，其中：\n",
    "\n",
    "`__init__()` 初始化数据集参数，这里设置了图像的存储目录、标签（通过读取标签 csv 文件）以及样本和标签的数据转换函数；\n",
    "`__len__()` 返回数据集中样本的个数；\n",
    "`__getitem__()` 映射型数据集的核心，根据给定的索引 `idx` 返回样本。这里会根据索引从目录下通过 `read_image` 读取图片和从 csv 文件中读取图片标签，并且返回处理后的图像和标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbef1e",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "前面的数据集 `Dataset` 类提供了一种按照索引访问样本的方式。不过在实际训练模型时，我们都需要先将数据集切分为很多的 mini-batches，然后按批 (batch) 将样本送入模型，并且循环这一过程，每一个完整遍历所有样本的循环称为一个 epoch。\n",
    "\n",
    "> 训练模型时，我们通常会在每次 epoch 循环开始前随机打乱样本顺序以缓解过拟合。\n",
    "\n",
    "Pytorch 提供了 DataLoader 类专门负责处理这些操作，除了基本的 `dataset`（数据集）和 `batch_size` （batch 大小）参数以外，还有以下常用参数：\n",
    "\n",
    "- `shuffle`：是否打乱数据集；\n",
    "- `sampler`：采样器，也就是一个索引上的迭代器；\n",
    "- `collate_fn`：批处理函数，用于对采样出的一个 batch 中的样本进行处理（例如前面提过的 Padding 操作）。\n",
    "\n",
    "例如，我们按照 `batch = 64` 遍历 Pytorch 自带的图像分类 FashionMNIST 数据集（每个样本是一张  的灰度图，以及分类标签），并且打乱数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "285f9b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "torch.Size([28, 28])\n",
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "print(img.shape)\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13f95e",
   "metadata": {},
   "source": [
    "### 数据加载顺序和 Sampler 类\n",
    "\n",
    "对于迭代型数据集来说，数据的加载顺序直接由用户控制，用户可以精确地控制每一个 batch 中返回的样本，因此不需要使用 `Sampler` 类。\n",
    "\n",
    "对于映射型数据集来说，由于索引可以不是整数，因此我们可以通过 `Sampler` 对象来设置加载时的索引序列，即设置一个索引上的迭代器。如果设置了 `shuffle` 参数，`DataLoader` 就会自动创建一个顺序或乱序的 `sampler`，我们也可以通过 `sampler` 参数传入一个自定义的 `Sampler` 对象。\n",
    "\n",
    "常见的 `Sampler` 对象包括序列采样器 `SequentialSampler` 和随机采样器 `RandomSampler`，它们都通过传入待采样的数据集来创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8cc941cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SequentialSampler, RandomSampler\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_sampler = RandomSampler(training_data)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, sampler=train_sampler)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, sampler=test_sampler)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "test_features, test_labels = next(iter(test_dataloader))\n",
    "print(f\"Feature batch shape: {test_features.size()}\")\n",
    "print(f\"Labels batch shape: {test_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0302c1",
   "metadata": {},
   "source": [
    "### 批处理函数 collate_fn\n",
    "\n",
    "批处理函数 `collate_fn` 负责对每一个采样出的 batch 中的样本进行处理。默认的 `collate_fn` 会进行如下操作：\n",
    "\n",
    "添加一个新维度作为 `batch` 维；\n",
    "自动地将 NumPy 数组和 Python 数值转换为 PyTorch 张量；\n",
    "保留原始的数据结构，例如输入是字典的话，它会输出一个包含同样键 (key) 的字典，但是将值 (value) 替换为 batched 张量（如果可以转换的话）。\n",
    "例如，如果样本是包含 3 通道的图像和一个整数型类别标签，即 (image, class_index)，那么默认的 `collate_fn` 会将这样的一个元组列表转换为一个包含 batched 图像张量和 batched 类别标签张量的元组。\n",
    "\n",
    "我们也可以传入手工编写的 `collate_fn` 函数以对数据进行自定义处理，例如前面我们介绍过的 `padding` 操作。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30e7b2",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "\n",
    "Pytorch 所有的模块（层）都是 nn.Module 的子类，神经网络模型本身就是一个模块，它还包含了很多其他的模块。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c3b0d",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "我们还是以前面加载的 FashionMNIST 数据库为例，构建一个神经网络模型来完成图像分类。模型同样继承自 `nn.Module` 类，通过 `__init__()` 初始化模型中的层和参数，在 `forward()` 中定义模型的操作，例如：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5ada1e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947aafe",
   "metadata": {},
   "source": [
    "可以看到，我们构建的模型首先将二维图像通过 `Flatten` 层压成一维向量，然后经过两个带有 `ReLU` 激活函数的全连接隐藏层，最后送入到一个包含 10 个神经元的分类器以完成 10 分类任务。我们还通过在最终输出前添加 `Dropout` 层来缓解过拟合。\n",
    "\n",
    "最终我们构建的模型会输出一个 10 维向量（每一维对应一个类别的预测值），与先前介绍过的 `pipeline` 模型一样，这里输出的是 `logits` 值，我们需要再接一个 `Softmax` 层来计算最终的概率值。\n",
    "\n",
    "下面我们构建一个包含四个伪二维图像的 `mini-batch` 来进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ceff85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "torch.Size([4, 10])\n",
      "tensor([[0.0791, 0.1027, 0.1009, 0.1097, 0.1056, 0.1105, 0.1037, 0.1067, 0.0936,\n",
      "         0.0874],\n",
      "        [0.0831, 0.1018, 0.0977, 0.1086, 0.1061, 0.0977, 0.1088, 0.1069, 0.0977,\n",
      "         0.0917],\n",
      "        [0.0964, 0.0997, 0.1017, 0.1042, 0.0964, 0.1127, 0.1020, 0.1086, 0.0914,\n",
      "         0.0867],\n",
      "        [0.0836, 0.1007, 0.1037, 0.1042, 0.0967, 0.1126, 0.1012, 0.1028, 0.0977,\n",
      "         0.0967]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([5, 6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "X = torch.rand(4, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab.size())\n",
    "print(pred_probab)\n",
    "y_pred = pred_probab.argmax(-1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435045b",
   "metadata": {},
   "source": [
    "可以看到，模型成功输出了维度为 $(4, 10)$ 的预测结果（每个样本输出一个 10 维的概率向量）。最后我们通过 $argmax$ 操作，将输出的概率向量转换为对应的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b185eab",
   "metadata": {},
   "source": [
    "### 优化模型参数\n",
    "\n",
    "在准备好数据、搭建好模型之后，我们就可以开始训练和测试（验证）模型了。正如前面所说，模型训练是一个迭代的过程，每一轮 epoch 迭代中模型都会对输入样本进行预测，然后对预测结果计算损失 (loss)，并求 loss 对每一个模型参数的偏导，最后使用优化器更新所有的模型参数。\n",
    "\n",
    "> **损失函数 (Loss function)** 用于度量预测值与答案之间的差异，模型的训练过程就是最小化损失函数。Pytorch 实现了很多常见的损失函数，例如用于回归任务的均方误差 (Mean Square Error) nn.MSELoss、用于分类任务的负对数似然 (Negative Log Likelihood) `nn.NLLLoss`、同时结合了 `nn.LogSoftmax` 和 `nn.NLLLoss` 的交叉熵损失 (Cross Entropy) [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) 等。\n",
    ">\n",
    "> **优化器 (Optimization)** 使用特定的优化算法（例如随机梯度下降），通过在每一个训练阶段 (step) 减少（基于一个 batch 样本计算的）模型损失来调整模型参数。Pytorch 实现了很多优化器，例如 SGD、ADAM、RMSProp 等。\n",
    "\n",
    "每一轮迭代 (Epoch) 实际上包含了两个步骤：\n",
    "\n",
    "- **训练循环 (The Train Loop)** 在训练集上进行迭代，尝试收敛到最佳的参数；\n",
    "- **验证/测试循环 (The Validation/Test Loop)** 在测试/验证集上进行迭代以检查模型性能有没有提升。\n",
    "\n",
    "具体地，在训练循环中，优化器通过以下三个步骤进行优化：\n",
    "\n",
    "- 调用 `optimizer.zero_grad()` 重设模型参数的梯度。默认情况下梯度会进行累加，为了防止重复计算，在每个训练阶段开始前都需要清零梯度；\n",
    "- 通过 `loss.backwards()` 反向传播预测结果的损失，即计算损失对每一个参数的偏导；\n",
    "- 调用 `optimizer.step()` 根据梯度调整模型的参数。\n",
    "\n",
    "下面我们选择交叉熵作为损失函数、选择 AdamW 作为优化器，完整的训练循环和测试循环实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fa1e9fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.945223  [ 6400/60000]\n",
      "loss: 0.804111  [12800/60000]\n",
      "loss: 0.566097  [19200/60000]\n",
      "loss: 0.874412  [25600/60000]\n",
      "loss: 0.460274  [32000/60000]\n",
      "loss: 0.451918  [38400/60000]\n",
      "loss: 1.093896  [44800/60000]\n",
      "loss: 0.694609  [51200/60000]\n",
      "loss: 0.528664  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.431817 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.541973  [ 6400/60000]\n",
      "loss: 0.644195  [12800/60000]\n",
      "loss: 0.531770  [19200/60000]\n",
      "loss: 0.714137  [25600/60000]\n",
      "loss: 0.447977  [32000/60000]\n",
      "loss: 0.597148  [38400/60000]\n",
      "loss: 0.922771  [44800/60000]\n",
      "loss: 0.586662  [51200/60000]\n",
      "loss: 0.505850  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.390008 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.481712  [ 6400/60000]\n",
      "loss: 0.594524  [12800/60000]\n",
      "loss: 0.549741  [19200/60000]\n",
      "loss: 0.731430  [25600/60000]\n",
      "loss: 0.328951  [32000/60000]\n",
      "loss: 0.490558  [38400/60000]\n",
      "loss: 0.903885  [44800/60000]\n",
      "loss: 0.529547  [51200/60000]\n",
      "loss: 0.537047  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.390692 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim=-1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ace243",
   "metadata": {},
   "source": [
    "可以看到，通过 3 轮迭代 (Epoch)，模型在训练集上的损失逐步下降、在测试集上的准确率逐步上升，证明优化器成功地对模型参数进行了调整，而且没有出现过拟合。\n",
    "\n",
    ">注意：一定要在预测之前调用 `model.eval()` 方法将 `dropout` 层和 `batch normalization` 层设置为评估模式，否则会产生不一致的预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7dec8",
   "metadata": {},
   "source": [
    "# 保存及加载模型\n",
    "\n",
    "在之前的文章中，我们介绍过模型类 Model 的保存以及加载方法，但如果我们只是将预训练模型作为一个模块（例如作为编码器），那么最终的完整模型就是一个自定义 Pytorch 模型，它的保存和加载就必须使用 Pytorch 预设的接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4c53e",
   "metadata": {},
   "source": [
    "## 保存和加载模型权重\n",
    "\n",
    "Pytorch 模型会将所有参数存储在一个状态字典 (state dictionary) 中，可以通过 `Model.state_dict()` 加载。Pytorch 通过 `torch.save()` 保存模型权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd220db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0405cd5",
   "metadata": {},
   "source": [
    "为了加载保存的权重，我们首先需要创建一个结构完全相同的模型实例，然后通过 `Model.load_state_dict()` 函数进行加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5690bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1b373",
   "metadata": {},
   "source": [
    "## 保存和加载完整模型\n",
    "\n",
    "上面存储模型权重的方式虽然可以节省空间，但是加载前需要构建一个结构完全相同的模型实例来承接权重。如果我们希望在存储权重的同时，也一起保存模型结构，就需要将整个模型传给 `torch.save()`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e06847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b8b4a",
   "metadata": {},
   "source": [
    "这样就可以直接从保存的文件中加载整个模型（包括权重和结构）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f3702",
   "metadata": {},
   "source": [
    "# 代码\n",
    "\n",
    "本章核心代码存储于：https://github.com/jsksxs360/How-to-use-Transformers/blob/main/train_model_FashionMNIST.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e7887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63b205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
